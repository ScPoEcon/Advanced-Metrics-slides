---
title: "ScPoEconometrics Advanced"
subtitle: "Recap 2"
author: "Florian Oswald"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "../../css/scpo.css", "../../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: ["../../libs/partials/header.html"]
---

layout: true

<div class="my-footer"><img src="../../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes,  ggforce, viridis, dplyr, magrittr, knitr, parallel)
# Define pink color
scpo_red <- "#d90502"
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  # dpi = 300,
  warning = F,
  message = F,
  dev = "svg",
  echo = FALSE
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_empty + theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)

```

```{R, colors, include = F}
# Define pink color
scpo_red <- "#d90502"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
```

# Recap 2

.pull-left[

* Last time, we refreshed our basic OLS knowledge

* Today we continue and look at more than one explanatory variable, and associated problems

]


.pull-right[

* But, why *more than one variable*?

* Like, **how many** other variables?

* And, above all: **which ones** ? 🤔


]

--

<br>
<br>

.center[We will remember what we meant by **a model**.]

---

# Back to the STAR Experiment

.left-thin[

* Remember what we learned about the [STAR Experiment](https://scpoecon.github.io/ScPoEconometrics/STAR.html)

* What is the causal impact of class size on test scores?

* $\text{score}_i = \beta_0 + \beta_1 \text{classize}_i + u_i$ ?
 
* We use a **model** to order our thoughts about how a causal impact is determined. 

]

--

.right-wide[
```{r,echo=FALSE}
knitr::include_graphics("star1-1.png")
```
]

---
class: middle

# Multiple Variables

Let's augment our model with more variables:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \beta_3 x_3 +u$$


---
# Spot the Difference 🕵️


```{r dag1,fig.height = 4}
library(ggdag)
library(dplyr)
dag = dagify(y ~ x1,
            y ~ x2,
            y ~ x3,
            x1 ~ x2,
            x3 ~ x2,
       coords = list(x = c("x2" = 0,"x1" = 1,"x3" = 1.0,"y" = 2),
                     y = c("x2" = 0,"x1" = 0.5,"x3" = -0.5,"y" = 0)))
p1 = dag %>%
  ggdag() + theme_dag()
p2 = dag %>%
  tidy_dagitty() %>%
      mutate(linetype = if_else(name =="x2", "dashed","solid")) %>% 
    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_dag_point() + 
      geom_dag_text(col = "white") +
    geom_dag_edges(aes(edge_linetype = linetype), show.legend = FALSE) + theme_dag()
cowplot::plot_grid(p1,p2,align = "v")
```

---

layout: false
class: title-slide-section-red, middle

# Omitted-variable bias

---
layout: true
# Omitted-variable bias

---


**Omitted-variable bias** (OVB) arises when we omit a variable that

1. affects our outcome variable $y$

2. correlates with an explanatory variable $x_j$

As it's name suggests, this situation leads to bias in our estimate of $\beta_j$.

--

**Note:** OVB Is not exclusive to multiple linear regression, but it does require multiple variables affect $y$.

---

**Example**

Let's imagine a simple model for the amount individual $i$ gets paid

$$ \text{Pay}_i = \beta_0 + \beta_1 \text{School}_i + \beta_2 \text{Male}_i + u_i $$

where

- $\text{School}_i$ gives $i$'s years of schooling
- $\text{Male}_i$ denotes an indicator variable for whether individual $i$ is male.

thus

- $\beta_1$: the returns to an additional year of schooling (*ceteris paribus*)
- $\beta_2$: the premium for being male (*ceteris paribus*)
<br>If $\beta_2 > 0$, then there is discrimination against women—receiving less pay based upon gender.

---
layout: true
# Omitted-variable bias

**Example, continued**

---

From our population model

$$ \text{Pay}_i = \beta_0 + \beta_1 \text{School}_i + \beta_2 \text{Male}_i + u_i $$

If a study focuses on the relationship between pay and schooling, _i.e._,

$$ \text{Pay}_i = \beta_0 + \beta_1 \text{School}_i + \left(\beta_2 \text{Male}_i + u_i\right) $$
$$ \text{Pay}_i = \beta_0 + \beta_1 \text{School}_i + \varepsilon_i $$

where $\varepsilon_i = \beta_2 \text{Male}_i + u_i$.

We used our exogeneity assumption to derive OLS' unbiasedness. But even if $\mathop{\boldsymbol{E}}\left[ u | X \right] = 0$, it is not true that $\mathop{\boldsymbol{E}}\left[ \varepsilon | X \right] = 0$ so long as $\beta_2 \neq 0$.

Specifically, $\mathop{\boldsymbol{E}}\left[ \varepsilon | \text{Male} = 1 \right] = \beta_2 + \mathop{\boldsymbol{E}}\left[ u | \text{Male} = 1 \right] \neq 0$.
--
 **Now OLS is biased.**

---

Let's try to see this result graphically.

```{R, gen ovb data, include = F, cache = T}
# Set seed
set.seed(12345)
# Sample size
n <- 1e3
# Parameters
beta0 <- 20; beta1 <- 0.5; beta2 <- 10
# Dataset
omit_df <- tibble(
  male = sample(x = c(F, T), size = n, replace = T),
  school = runif(n, 3, 9) - 3 * male,
  pay = beta0 + beta1 * school + beta2 * male + rnorm(n, sd = 7)
)
lm_bias <- lm(pay ~ school, data = omit_df)
bb0 <- lm_bias$coefficients[1] %>% round(1)
bb1 <- lm_bias$coefficients[2] %>% round(1)
lm_unbias <- lm(pay ~ school + male, data = omit_df)
bu0 <- lm_unbias$coefficients[1] %>% round(1)
bu1 <- lm_unbias$coefficients[2] %>% round(1)
bu2 <- lm_unbias$coefficients[3] %>% round(1)
```

The population model:

$$ \text{Pay}_i = `r beta0` + `r beta1` \times \text{School}_i + `r beta2` \times \text{Male}_i + u_i $$

Our regression model that suffers from omitted-variable bias:

$$ \text{Pay}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \text{School}_i + e_i $$

Finally, imagine that women, on average, receive more schooling than men.

---
layout: true
# Omitted-variable bias
**Example, continued:** $\text{Pay}_i = `r beta0` + `r beta1` \times \text{School}_i + `r beta2` \times \text{Male}_i + u_i$

---

The relationship between pay and schooling.

```{R, plot ovb 1, echo = F, dev = "svg", fig.height = 4.0}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, color = "black", alpha = 0.4, shape = 16) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)
```
---
count: false
Biased regression estimate: $\widehat{\text{Pay}}_i = `r bb0` + `r bb1` \times \text{School}_i$

```{R, plot ovb 2, echo = F, dev = "svg", fig.height = 4.0}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, color = "black", alpha = 0.4, shape = 16) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_smooth(se = F, color = "orange", method = lm) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)
```

---
count: false
Recalling the omitted variable: Gender **female** and .hi-slate[male]

```{R, plot ovb 3, echo = F, dev = "svg", fig.height = 4.0}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = male, shape = male)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.5, size = 1) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(scpo_red, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))
```

---
count: false
Recalling the omitted variable: Gender **female** and .hi-slate[male]

```{R, plot ovb 4, echo = F, dev = "svg", fig.height = 4.0}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = male, shape = male)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.2, size = 1) +
geom_abline(
  intercept = lm_unbias$coefficients[1],
  slope = lm_unbias$coefficients[2],
  color = scpo_red, size = 1
) +
geom_abline(
  intercept = lm_unbias$coefficients[1] + lm_unbias$coefficients[3],
  slope = lm_unbias$coefficients[2],
  color = "darkslategrey", size = 1
) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(scpo_red, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))
```

---
count: false
Unbiased regression estimate: $\widehat{\text{Pay}}_i = `r bu0` + `r bu1` \times \text{School}_i + `r bu2` \times \text{Male}_i$

```{R, plot ovb 5, echo = F, dev = "svg", fig.height = 4.0}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = male, shape = male)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.2, size = 1) +
geom_abline(
  intercept = lm_unbias$coefficients[1],
  slope = lm_unbias$coefficients[2],
  color = scpo_red, size = 1
) +
geom_abline(
  intercept = lm_unbias$coefficients[1] + lm_unbias$coefficients[3],
  slope = lm_unbias$coefficients[2],
  color = "darkslategrey", size = 1
) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(scpo_red, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))
```
---
layout: false
class: clear, middle, center

```{R, venn2, dev = "svg", echo = F,eval = F}
# Line types (order: x1, x2, x3, y)
venn_lines <- c("solid", "dotted", "dotted", "solid")
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans Book", parse = T) +
annotate(
  x = -5.5, y = 3.3,
  geom = "text", label = "Omitted variables", size = 9, family = "Fira Sans Book", hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

```{r}

```
---
layout: false

# Omitted-variable bias

## Solutions

1. Don't omit variables 😜

2. Instrumental variables and two-stage least squares (coming soon): If we could find something that **only** affects $x_1$ but *not* the omitted variable, we can make progress!

3. Use multiple observations for the same unit $i$: panel data.

**Warning:** There are situations in which neither solution is possible.

--

1. Proceed with caution (sometimes you can sign the bias).

2. The key is to have a mental map of *should* belong to the model. 



---


---
layout: false
class: separator, middle

# Interpreting coefficients


---
# Interpreting coefficients
## Continuous variables

Consider the relationship

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + u_i $$

where

- $\text{Pay}_i$ is a continuous variable measuring an individual's pay
- $\text{School}_i$ is a continuous variable that measures years of education

---

# Interpreting coefficients
**Interpretations**

- $\beta_0$: the $y$-intercept, _i.e._, $\text{Pay}$ when $\text{School} = 0$
- $\beta_1$: the expected increase in $\text{Pay}$ for a one-unit increase in $\text{School}$

---
# Interpreting coefficients
## Continuous variables


Consider the model

$$ y = \beta_0 + \beta_1 \, x + u $$

Differentiate the model:

$$ \dfrac{dy}{dx} = \beta_1 $$


---
# Interpreting coefficients
## Categorical variables

Consider the relationship

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{Female}_i + u_i $$

where

- $\text{Pay}_i$ is a continuous variable measuring an individual's pay
- $\text{Female}_i$ is a binary/indicator variable taking $1$ when $i$ is female

---
# Interpreting coefficients
**Interpretations**

- $\beta_0$: the expected $\text{Pay}$ for males (_i.e._, when $\text{Female} = 0$)
- $\beta_1$: the expected difference in $\text{Pay}$ between females and males
- $\beta_0 + \beta_1$: the expected $\text{Pay}$ for females

---
# Interpreting coefficients
## Categorical variables

Derivations

$$
\begin{aligned}
 \mathop{\boldsymbol{E}}\left[ \text{Pay} | \text{Male} \right] &=
 \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1\times 0 + u_i \right] \\
 &= \mathop{\boldsymbol{E}}\left[ \beta_0 + 0 + u_i \right] \\
 &= \beta_0
\end{aligned}
$$

--

$$
\begin{aligned}
 \mathop{\boldsymbol{E}}\left[ \text{Pay} | \text{Female} \right] &=
 \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1\times 1 + u_i \right] \\
 &= \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 + u_i \right] \\
 &= \beta_0 + \beta_1
\end{aligned}
$$

--

**Note:** If there are no other variables to condition on, then $\hat{\beta}_1$ equals the difference in group means, _e.g._, $\overline{x}_\text{Female} - \overline{x}_\text{Male}$.

--

**Note<sub>2</sub>:** The *holding all other variables constant* interpretation also applies for categorical variables in multiple regression settings.

---
# Interpreting coefficients
## Categorical variables

$y_i = \beta_0 + \beta_1 x_i + u_i$ for binary variable $x_i = \{\color{#314f4f}{0}, \, \color{#d90502}{1}\}$

```{R, cat data, include = F}
# Set seed
set.seed(1235)
# Sample size
n <- 5e3
# Generate data
cat_df <- tibble(
  x = sample(x = c(0, 1), size = n, replace = T),
  y = 3 + 7 * x + rnorm(n, sd = 2)
)
# Regression
cat_reg <- lm(y ~ x, data = cat_df)
```

```{R, dat plot 1, echo = F, dev = "svg", fig.height = 3.5}
set.seed(12345)
ggplot(data = cat_df, aes(x = x, y = y, color = as.factor(x))) +
geom_jitter(width = 0.3, size = 1.5, alpha = 0.5) +
scale_color_manual(values = c("darkslategrey", scpo_red)) +
theme_empty
```

---
# Interpreting coefficients
## Categorical variables

$y_i = \beta_0 + \beta_1 x_i + u_i$ for binary variable $x_i = \{\color{#314f4f}{0}, \, \color{#d90502}{1}\}$

```{R, dat plot 2, echo = F, dev = "svg", fig.height = 3.5}
set.seed(12345)
ggplot(data = cat_df, aes(x = x, y = y, color = as.factor(x))) +
geom_jitter(width = 0.3, size = 1.5, alpha = 0.5) +
scale_color_manual(values = c("darkslategrey", scpo_red)) +
geom_hline(yintercept = cat_reg$coefficients[1], size = 1, color = "darkslategrey") +
geom_hline(yintercept = cat_reg$coefficients[1] + cat_reg$coefficients[2], size = 1, color = scpo_red) +
annotate(
  geom = "text",
  x = 0.5,
  y = -1 + cat_reg$coefficients[1],
  label = TeX("$\\hat{\\beta}_0 = \\bar{\\mathrm{Group}_0}$"),
  size = 7
) +
annotate(
  geom = "text",
  x = 0.5,
  y = 1 + cat_reg$coefficients[1] + cat_reg$coefficients[2],
  label = TeX("$\\hat{\\beta}_0 + \\hat{\\beta}_1 = \\bar{\\mathrm{Group}_1}$"),
  size = 7,
  color = scpo_red
) +
theme_empty
```
---
# Interpreting coefficients
## Interactions

Interactions allow the effect of one variable to change based upon the level of another variable.

**Examples**

1. Does the effect of schooling on pay change by gender?

1. Does the effect of gender on pay change by race?

1. Does the effect of schooling on pay change by experience?

---
# Interpreting coefficients
## Interactions

Previously, we considered a model that allowed women and men to have different wages, but the model assumed the effect of school on pay was the same for everyone:

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Female}_i + u_i $$

but we can also allow the effect of school to vary by gender:

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Female}_i + \beta_3 \, \text{School}_i\times\text{Female}_i + u_i $$

---
# Interpreting coefficients
## Interactions

The model where schooling has the same effect for everyone (**<font color="#e64173">F</font>** and **<font color="#314f4f">M</font>**):

```{R, int data, include = F, cache = T}
# Set seed
set.seed(12345)
# Sample size
n <- 1e3
# Parameters
beta0 <- 20; beta1 <- 0.5; beta2 <- 10; beta3 <- 3
# Dataset
int_df <- tibble(
  male = sample(x = c(F, T), size = n, replace = T),
  school = runif(n, 3, 9) - 3 * male,
  pay = beta0 + beta1 * school + beta2 * male + rnorm(n, sd = 7) + beta3 * male * school
)
reg_noint <- lm(pay ~ school + male, int_df)
reg_int <- lm(pay ~ school + male + school:male, int_df)
```

```{R, int plot 1, echo = F, dev = "svg", fig.height = 3.5}
ggplot(data = int_df, aes(x = school, y = pay)) +
geom_point(aes(color = male, shape = male), size = 2.5) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_abline(
  intercept = reg_noint$coefficients[1] + reg_noint$coefficients[3],
  slope = reg_noint$coefficients[2],
  color = "darkslategrey", size = 1, alpha = 0.8
) +
geom_abline(
  intercept = reg_noint$coefficients[1],
  slope = reg_noint$coefficients[2],
  color = scpo_red, size = 1, alpha = 0.8
) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(scpo_red, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))
```

---
# Interpreting coefficients
## Interactions

The model where schooling's effect can differ by gender (**<font color="#e64173">F</font>** and **<font color="#314f4f">M</font>**):

```{R, int plot 2, echo = F, dev = "svg", fig.height = 3.5}
ggplot(data = int_df, aes(x = school, y = pay)) +
geom_point(aes(color = male, shape = male), size = 2.5) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_abline(
  intercept = reg_noint$coefficients[1] + reg_noint$coefficients[3],
  slope = reg_noint$coefficients[2],
  color = "darkslategrey", size = 0.75, alpha = 0.2
) +
geom_abline(
  intercept = reg_noint$coefficients[1],
  slope = reg_noint$coefficients[2],
  color = scpo_red, size = 0.75, alpha = 0.2
) +
geom_abline(
  intercept = reg_int$coefficients[1] + reg_int$coefficients[3],
  slope = reg_int$coefficients[2] + reg_int$coefficients[4],
  color = "darkslategrey", size = 1, alpha = 0.8
) +
geom_abline(
  intercept = reg_int$coefficients[1],
  slope = reg_int$coefficients[2],
  color = scpo_red, size = 1, alpha = 0.8
) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(scpo_red, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))
```

---
# Interpreting coefficients
## Interactions

Interpreting coefficients can be a little tricky with interactions, but the key<sup>.pink[†]</sup> is to carefully work through the math.

.footnote[.pink[†] As is often the case with econometrics.]

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Female}_i + \beta_3 \, \text{School}_i\times\text{Female}_i + u_i $$

Expected returns for an additional year of schooling for women:

$$
\begin{aligned}
 \mathop{\boldsymbol{E}}\left[ \text{Pay}_i | \text{Female} \land \text{School} = \ell + 1 \right] -
    \mathop{\boldsymbol{E}}\left[ \text{Pay}_i | \text{Female} \land \text{School} = \ell \right] &= \\
 \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 (\ell+1) + \beta_2 + \beta_3 (\ell + 1) + u_i \right] -
    \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 \ell + \beta_2 + \beta_3 \ell + u_i  \right] &= \\
 \beta_1 + \beta_3
\end{aligned}
$$

--

Similarly, $\beta_1$ gives the expected return to an additional year of schooling for men. Thus, $\beta_3$ gives the **difference in the returns to schooling** for women and men.

---
# Interpreting coefficients
## Log-linear specification

In economics, you will frequently see logged outcome variables with linear (non-logged) explanatory variables, _e.g._,

$$ \log(\text{Pay}_i) = \beta_0 + \beta_1 \, \text{School}_i + u_i $$

This specification changes our interpretation of the slope coefficients.

**Interpretation**

- A one-unit increase in our explanatory variable increases the outcome variable by approximately $\beta_1\times 100$ percent.

- *Example:* An additional year of schooling increases pay by approximately 3 percent (for $\beta_1 = 0.03$).

---
# Interpreting coefficients
## Log-linear specification


Consider the log-linear model

$$ \log(y) = \beta_0 + \beta_1 \, x + u $$

and differentiate

$$ \dfrac{dy}{y} = \beta_1 dx $$

So a marginal change in $x$ (_i.e._, $dx$) leads to a $\beta_1 dx$ **percentage change** in $y$.

---
# Interpreting coefficients
## Log-linear specification

Because the log-linear specification comes with a different interpretation, you need to make sure it fits your data-generating process/model.

Does $x$ change $y$ in levels (_e.g._, a 3-unit increase) or percentages (_e.g._, a 10-percent increase)?

--

_I.e._, you need to be sure an exponential relationship makes sense:

$$ \log(y_i) = \beta_0 + \beta_1 \, x_i + u_i \iff y_i = e^{\beta_0 + \beta_1 x_i + u_i} $$

---
# Interpreting coefficients
## Log-linear specification

```{R, log linear plot, echo = F, cache = T, dev = "svg", fig.height = 4}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
ll_df <- tibble(
  x = runif(n, 0, 3),
  y = exp(-100 + 0.75 * x + rnorm(n, sd = 0.5))
)
# Plot
ggplot(data = ll_df, aes(x = x, y = y)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = scpo_red, se = F) +
xlab("x") +
ylab("y") +
theme_axes
```


---
# Interpreting coefficients
## Log-log specification

Similarly, econometricians frequently employ log-log models, in which the outcome variable is logged *and* at least one explanatory variable is logged

$$ \log(\text{Pay}_i) = \beta_0 + \beta_1 \, \log(\text{School}_i) + u_i $$

**Interpretation:**

- A one-percent increase in $x$ will lead to a $\beta_1$ percent change in $y$.
- Often interpreted as an elasticity.

---
# Interpreting coefficients
## Log-log specification


Consider the log-log model

$$ \log(y) = \beta_0 + \beta_1 \, \log(x) + u $$

and differentiate

$$ \dfrac{dy}{y} = \beta_1 \dfrac{dx}{x} $$

which says that for a one-percent increase in $x$, we will see a $\beta_1$ percent increase in $y$. As an elasticity:

$$ \dfrac{dy}{dx} \dfrac{x}{y} = \beta_1 $$

---
# Interpreting coefficients
## Log-linear with a binary variable

**Note:** If you have a log-linear model with a binary indicator variable, the interpretation for the coefficient on that variable changes.

Consider

$$ \log(y_i) = \beta_0 + \beta_1 x_1 + u_i $$

for binary variable $x_1$.

The interpretation of $\beta_1$ is now

- When $x_1$ changes from 0 to 1, $y$ will change by $100 \times \left( e^{\beta_1} -1 \right)$ percent.
- When $x_1$ changes from 1 to 0, $y$ will change by $100 \times \left( e^{-\beta_1} -1 \right)$ percent.


---
# Interpreting coefficients
## Log-log specification

```{R, log log plot, echo = F, cache = T, dev = "svg", fig.height = 4}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
log_df <- tibble(
  x = runif(n, 0, 10),
  y = exp(3 * log(x) + rnorm(n, sd = 0.5))
)
# Plot
ggplot(data = log_df, aes(x = x, y = y)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = scpo_red, se = F) +
xlab("x") +
ylab("y") +
theme_axes
```

---
layout: true
# Additional topics

---
class: inverse, middle

---
## Inference *vs.* prediction

So far, we've focused mainly **statistical inference**—using estimators and their distributions properties to try to learn about underlying, unknown population parameters.

$$ y\_i = \color{#e64173}{\hat{\beta}\_{0}} + \color{#e64173}{\hat{\beta\_1}} \, x\_{1i} + \color{#e64173}{\hat{\beta\_2}} \, x\_{2i} + \cdots + \color{#e64173}{\hat{\beta}\_{k}} \, x\_{ki} + e\_i $$

--

**Prediction** includes a fairly different set of topics/tools within econometrics (and data science/machine learning)—creating models that accurately estimate individual observations.

$$ \color{#e64173}{\hat{y}\_i} = \mathop{\hat{f}}\left( x_1,\, x_2,\, \ldots x_k \right) $$

---
## Inference *vs.* prediction

Succinctly

- **Inference:** causality, $\hat{\beta}_k$ (consistent and efficient), standard errors/hypothesis tests for $\hat{\beta}_k$, generally OLS

- **Prediction:**<sup>.pink[†]</sup> correlation, $\hat{y}_i$ (low error), model selection, nonlinear models are much more common

.footnote[
.pink[†] Includes forecasting.
]

---
## Treatment effects and causality

Much of modern (micro)econometrics focuses on causally estimating (*identifying*) the effect of programs/policies, _e.g._,

- [Government shutdowns](https://www.sciencedirect.com/science/article/pii/S004727271830118X)
- [The minimum wage](https://www.jstor.org/stable/2118030)
- [Recreational-cannabis legalization](https://pages.uoregon.edu/bchansen/working.html)
- [Salary-history bans](http://www.drewmcnichols.com/research)
- [Preschool](https://amstat.tandfonline.com/doi/abs/10.1198/016214508000000841#.XD4PVy2ZNO4)
- [The Clean Water Act](https://academic.oup.com/qje/article-abstract/134/1/349/5092609)

--

In this literature, the program is often a binary variable, and we place high importance on finding an unbiased estimate for the program's effect, $\hat{\tau}$.

$$ \text{Outcome}_i = \beta_0 + \tau \, \text{Program}_i + u_i $$

---
layout: false
class: title-slide-final, middle
background-image: url(../../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/Advanced-Metrics-slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |




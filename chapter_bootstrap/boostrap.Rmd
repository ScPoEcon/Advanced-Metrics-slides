---
title: "ScPoEconometrics"
subtitle: "Confidence Intervals and The Bootstrap"
author: "Florian Oswald"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(magrittr)

```

# Where Did We Stop Last Week?

.pull-left[
* Last week we learned about *sampling distributions*

* We took repeated samples from a population (Fusilli or plastic balls) and computed an **estimate** of the sample proportion $\hat{p}$ over and over again

* We took 1000 (!) samples and looked how the sampling distribution evolved as we increased sample sizes from $n=25$ to $n=100$. 

* Our estimates became *more precise* as $n$ increased. 

]

--

.pull-right[
* We introduced notation: 
    * population size $N$, 
    * *point estimate* (like $\hat{p}$), 
    * standard error *of* an estimate

* We said *random sampling produces **unbiased** estimates*.

* Our sample statistics provided **good guesses** about the true population parameters of interest.
]



---
background-image: url(https://media.giphy.com/media/hXG93399r19vi/giphy.gif)
background-position: 10% 50%
background-size: 500px

# Reality Check

.pull-right[
* Who could ever take **1000** samples for any study?!

* Also, we **knew** the true population proportion of red balls/green Fusilli!

* If we knew in real life, we wouldn't have to take samples altogether, would we?

* So what on earth was all of this good for? Fun Only?! `r emo::ji("anguished")`. 
]

---

# Real Life Sampling

.pull-left[
* In reality we only get to take **one** sample from the population.

* Still we *know* now that there exists a *sampling distribution*. 

* In other words: we *know* that there is sampling variation.

* With one sample only, how can we know what it looks like?
]

--

.pull-right[
* There are two approaches:
    1. Theory: Use mathematical formulas to derive the sampling distributions for our estimators under certain conditions.
    1. Simulation. We can use the **Bootstrap** method to *reconstruct* the sampling distribution.
    
* We'll focus on the bootstrap now and come back to the maths approach later.

* But first: what is a *bootstrap*?
]

---
background-image: url(https://upload.wikimedia.org/wikipedia/commons/9/9b/Zentralbibliothek_Solothurn_-_M%C3%BCnchhausen_zieht_sich_am_Zopf_aus_dem_Sumpf_-_a0400.tif)
background-position: 12% 50%
background-size: 400px

# Baron von Münchhausen (not)

.pull-right[
* The ethymology of *bootstrapping* is apparently [wrongly attributed](https://en.wikipedia.org/wiki/Bootstrapping) to the Baron of Münchhausen (who pulled himself out of the swamp by his own pigtail)

* The idea is to *pull oneself up by one's own bootstraps*.

* It sounds as if one had super-powers.

* It's a simple but powerful idea. We just *pretend* that our sample **is** the population. Then we repeatedly *resample* from it. 
]

---
background-image: url(https://media.giphy.com/media/pPhyAv5t9V8djyRFJH/giphy.gif)
background-position: 12% 50%
background-size: 700px

.right-thin[
<br>
<br>
<br>

* `r emo::ji("exploding_head")`


]


---
background-image: url(https://media.giphy.com/media/pPhyAv5t9V8djyRFJH/giphy.gif)
background-position: 12% 50%
background-size: 700px

.right-thin[
<br>
<br>
<br>

* `r emo::ji("exploding_head")`

* Ok, let's do a little hands-on activity first.

* You'll be fine. 

* `r emo::ji("sunglasses")`
]

---

# What's the Average Age of US Pennies?

* [`Moderndive`](https://moderndive.com) went for us to the bank.

* First, They got a roll of 50 pennies.


![:scale 40%](../img/photos/md-bank.jpg)![:scale 40%](../img/photos/md-bank2.jpg)

---

# What's the Average Age of US Pennies?


.left-wide[
* Then, they recorded the year each penny was minted:


![:scale 70%](../img/photos/md-pennies.jpg)
]

.thin-right[
* This data is in `moderndive`

```{r}
library(moderndive)
pennies_sample
```
]

---

# Analyzing Pennies

.pull-left[

* Let's assume this is a *representative, random* sample of all US pennies.

* Let's look at it's distribution and compute the mean `year` in the sample.

* Given randomness, that estimate should come close to the truth.

]

--

.pull-right[
```{r,echo = FALSE,fig.height=3,warning=FALSE,message = FALSE}
library(ggplot2)
library(dplyr)
ggplot(pennies_sample, aes(x = year)) +
  geom_histogram(binwidth = 10, color = "white")
```

```{r}
pennies_sample %>% 
  summarize(mean_year = mean(year))
```
]

---

# Pennies Sample Estimates

.pull-left[
* So far, this is very similar to Fusilli or Balls, where we estimated $p$ via $\hat{p}$

* Here, we estimate the population average $\mu$ via the **sample mean** $\bar{x}$.

* Our best guess about $\mu$ is thus about 1995.

* But what about sampling variation?

* If we went back to the bank for another roll of pennies, would we end up with the same number for $\bar{x}$?
]

--

.pull-right[
* Last week we drew 1000 samples from our population of (virtual) balls.

* That gave rise to a *distribution* of estimates $\hat{p}$.

* We could go back to the bank 1000 times, but `r emo::ji("thinking")`

* We can also use our *single* sample to learn about the sampling distribution. 

* It's called **resampling with replacement**.
]

---

# Resampling Once

.pull-left[

1. Printed out equally-sized paper slips representing the 50 pennies in the sample.

1. Put them in a paper cup or similar 

1. Shake cup to mix the slips, then take out a random slip.

1. Record year on slip.

1. **Put the slip back** into the hat!

1. Repeat 3.-4. for 49 times.
]

--

.pull-right[

![](../img/photos/bootstrap1.JPG)

]

---

# Collect the Bootstrapped Sample

* Let's put the recorded years into a `tibble`!

```{r}
pennies_resample <- tibble(
  year = c(1976, 1962, 1976, 1983, 2017, 2015, 2015, 1962, 2016, 1976, 
           2006, 1997, 1988, 2015, 2015, 1988, 2016, 1978, 1979, 1997, 
           1974, 2013, 1978, 2015, 2008, 1982, 1986, 1979, 1981, 2004, 
           2000, 1995, 1999, 2006, 1979, 2015, 1979, 1998, 1981, 2015, 
           2000, 1999, 1988, 2017, 1992, 1997, 1990, 1988, 2006, 2000)
)
```

* And now let's look at both original and bootstrapped sample!

---

# Comparing Resampled Data To Original Sample

.left-wide[
```{r,echo = FALSE, fig.height = 5}
p1 = ggplot(pennies_resample, aes(x = year)) +
  geom_histogram(binwidth = 10, color = "white",boundary = 1990) +
  labs(title = "Resample of 50 pennies")
p2 = ggplot(pennies_sample, aes(x = year)) +
  geom_histogram(binwidth = 10, color = "white",boundary = 1990) +
  labs(title = "Original sample of 50 pennies")
cowplot::plot_grid(p1,p2)
```
]

--

.right-thin[
Also, comparing both means:

```{r}
pennies_sample %>% 
  summarize(mean_year = 
              mean(year))
```

```{r}
pennies_resample %>% 
  summarize(mean_year = 
              mean(year))
```
]



---

# Resampling 11 Times

.pull-left[
* Let's do this together now!

* Everybody produce one re-sample of 50 draws from their set of paper slips.

* Don't forget to **replace** the slip and mix!



* We'll collect data via this [shared google sheet](https://docs.google.com/spreadsheets/d/1cBefpVyP3q2SortQD3TJ_XFQ2kJ5iSxM6qjwTvqcooY/edit?usp=sharing)

* In class, read this data with
    ```{r}
    url <- "https://docs.google.com/spreadsheets/d/e/2PACX-1vTb_D6Qczq_WLN-VmhG0-xTxh4Kb_e4A_5BPr2p9wtLN9UBB77yMDDKU3nGe34xVvadzjUQCt9jpWj_/pub?gid=0&single=true&output=csv"
    # pennies_our_sample <- readr::read_csv(url)
    ```
]

.pull-right[
![](../img/photos/bootstrap2.JPG)

]

---

# `moderndive` Resampled Data

.pull-left[
* Our friends collected 35 Re-samples.

* The data is collected in
    ```{r}
    pennies_resamples
    ```


]

.pull-right[
* And we want to compute the average `year` from each sample:
    ```{r}
    resampled_means <- pennies_resamples %>% 
      group_by(name) %>% 
      summarize(mean_year = mean(year))
    resampled_means
    ```

]

---

# Visualizing Our (Manual) Bootstrap Distribution

.left-wide[
```{r,echo = FALSE,fig.height=5}
ggplot(resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
  labs(x = "Sampled mean year")
```
]

.right-thin[
## Discuss:

1. Shape

2. Central Location

of this!
]


---

# Recap

.pull-left[
* We demonstrated **bootstrapping**. We illustrated sampling distributions like in the previous chapter by *resampling* from a single sample, rather than from the population.

* We obtained the **bootstrap distribution**, which approximates the true *sampling distribution*

* Next up:

]

--

.pull-right[
* We will again re-do the exercise virtually on your computers.

* We will define the concept of **confidence interval**

* We will introduce the `infer` package to perform *inference* in a `dplyr` pipeline.

]



---

# Virtual Bootstrapping


.pull-left[
* Let's start with a single resample.

* That's like taking 50 paper slips out of our hat before.
    ```{r}
    virtual_resample <- pennies_sample %>% 
      rep_sample_n(size = 50, replace = TRUE)
    ```
]

--

.pull-right[
* notice we set `replace = TRUE`!
    ```{r}
    virtual_resample
    ```
]

---

# Resampling 35 Times

.pull-left[
First, compute 35 replicates:
```{r}
virtual_resamples <- pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 35)
virtual_resamples
```
]

--

.pull-right[
Then compute the mean by `replicate`!

```{r}
virtual_resampled_means <- virtual_resamples %>% 
  group_by(replicate) %>% 
  summarize(mean_year = mean(year))
virtual_resampled_means
```
]

---

# Resampling 1000 times


```{r}
virtual_resampled_means <- pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>% 
  group_by(replicate) %>% 
  summarize(mean_year = mean(year))
virtual_resampled_means
```

---

# Distribution of 1000 Resamples

```{r,echo = FALSE,fig.height = 5}
ggplot(virtual_resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
  labs(x = "sample mean")
```


---
layout: false
class: title-slide-section-red, middle

# Confidence Intervals



---
layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

# Confidence Intervals (CIs)

.pull-left[
* A CI gives us a *range* of plausible values for any given estimate.

* In order to construct a CI, we need 
    1. an underlying *sampling distribution*, either boostrapped as here, or derived from theory.
    2. to specify a **confidence level**, like 90% or 95% etc. Greater level implies wider CIs.
    
* There are two methods: the **percentile method** and the **standard error method**.
]

--

.pull-right[

1. the **percentile method** cuts the bootstrap distribution below the 2.5% and 97.5% percentile (thus it selects the middle 95%).

2. The **standard error method** assumes that the sampling distribution is approximately normal, and uses a corresponding formula for the width of the CI.

]


---

# Percentile Method

.left-wide[
```{r,echo = FALSE,fig.height=5}
library(infer)
x_bar <- pennies_sample %>% 
  summarize(mean_year = mean(year))
SE = virtual_resampled_means %>% 
  summarize(SE = sd(mean_year))
percentile_ci <- virtual_resampled_means %>% 
  rename(stat = mean_year) %>% 
  get_ci(level = 0.95, type = "percentile")
ggplot(virtual_resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1988) +
  labs(x = "Resample sample mean") +
  scale_x_continuous(breaks = seq(1988, 2006, 2)) +
  geom_vline(xintercept = percentile_ci[[1, 1]], size = 1) +
  geom_vline(xintercept = percentile_ci[[1, 2]], size = 1)

```
]

.right-thin[

* We cut the boostrap distribution at the 2.5% and 97.5% quantiles

* Those values are `r percentile_ci[[1, 1]]` and `r percentile_ci[[1, 2]]`

]

---

# Standard Error Method

.left-wide[
* We need mean $\bar{x}$ and *standard error* (SE) of the mean:

* If approximately normal, 95% of observations fall within $\pm 1.96$ standard deviations of the mean.

* We obtain the CIs as follows:

$$\begin{align} x \pm 1.96 \cdot SE &= (\bar{x} -  1.96 \cdot SE, \bar{x} +  1.96 \cdot SE)\\ &= (`r x_bar[1,1]` -  1.96 \cdot `r round(SE[1,1],2)`, `r x_bar[1,1]` +  1.96 \cdot `r round(SE[1,1],2)`)\\ &= (`r x_bar[1,1] - 1.96*round(SE[1,1],2)`, `r x_bar[1,1] + 1.96*round(SE[1,1],2)`) \end{align}$$

* Only valid, if sampling distribution is **approximately normal**!

]

.right-thin[
```{r}
x_bar <- pennies_sample %>% 
  summarize(mean_year = 
              mean(year))
SE = virtual_resampled_means %>% 
  summarize(SE = 
              sd(mean_year))
SE
```

* Remember that we called this the *standard error* (SE) of the mean.

]


---

# Comparing Percentile and Standard Error Method

.left-wide[
```{r,echo = FALSE,fig.height=5}
standard_error_ci <- virtual_resampled_means %>% 
  rename(stat = mean_year) %>% 
  get_ci(type = "se", point_estimate = x_bar)

# bootstrap SE value as scalar
bootstrap_se <- virtual_resampled_means %>% 
  summarize(se = sd(mean_year)) %>% 
  pull(se)

## ----percentile-and-se-method, echo=FALSE, message=FALSE, fig.cap="Comparing two 95 percent confidence interval methods."----
both_CI <- bind_rows(
  percentile_ci %>% tidyr::gather(endpoint, value) %>% mutate(type = "percentile"),
  standard_error_ci %>% tidyr::gather(endpoint, value) %>% mutate(type = "SE")
)
ggplot(virtual_resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1988) +
  labs(x = "sample mean", title = "Percentile method CI (solid lines), SE method CI (dashed lines)") +
  scale_x_continuous(breaks = seq(1988, 2006, 2)) +
  geom_vline(xintercept = percentile_ci[[1, 1]], size = 1) +
  geom_vline(xintercept = percentile_ci[[1, 2]], size = 1) + 
  geom_vline(xintercept = standard_error_ci[[1, 1]], linetype = "dashed", size = 1) +
  geom_vline(xintercept = standard_error_ci[[1, 2]], linetype = "dashed", size = 1)
```
]

.right-thin[
* The *percentile method* is based on the simulated bootstrap distribution. Shape of distribution is irrelevant.

* The *standard error method* is based on *assuming* that the sampling dist. is approximately normal.

]

---

# Constructing Confidence Intervals With `infer`

.pull-left[

* The `infer` package is useful for **inference**.

* It relies on the bootstrap just like we introduced it above.

* A typical `infer` pipeline looks like this:

```{r,eval = FALSE}
library(infer)
pennies_sample %>%
  specify(response = year) %>%
  generate(reps = 35,type = "bootstrap") %>%
  calculate(stat = "mean") %>%
  visualize()
```
]

.pull-right[
<br>
<br>
![:scale 100%](../img/photos/visualize.png)
]

---

# Percentile Method with `infer`

.pull-left[
* Let's start with creating a bootstrap distribution:
    ```{r}
    bootstrap_distribution <- pennies_sample %>% 
      specify(response = year) %>% 
      generate(reps = 1000, type = "bootstrap") %>% 
      calculate(stat = "mean")
    bootstrap_distribution
    ```

]

--

.pull-right[
* Then, let's visualize it!

```{r,fig.height=5}
visualize(bootstrap_distribution)
```
]

---

# Percentile Method with `infer`

.pull-left[
* We can easily get the CI like this:
    ```{r}
    percentile_ci <- bootstrap_distribution %>% 
      get_confidence_interval(level = 0.95, 
                              type = "percentile")
    percentile_ci
    ```

* Just set `type = "se"` for standard error method!

* What's best is the visualization though! `r emo::ji("tada")`
]

--

.pull-right[
```{r,fig.height=5}
visualize(bootstrap_distribution) + 
  shade_confidence_interval(endpoints = 
                              percentile_ci)
```

* Or shorter `shade_ci()`.
]

---

# Interpreting CIs

.pull-left[

* Now we want to know: is the true value contained in the CI?

* Let's go back to the red/white balls in `bowl`.

* Given this **is** the population, we can compute the true $p$:

```{r}
bowl %>% 
  summarize(p_red = mean(color == "red"))
```
]

--

.pull-right[
* In reality, again, we *don't know* $p$, and that is why we go through all this estimation trouble.

* Let's take one of the samples taken from the bowl, `bowl_sample_1`
    ```{r}
    prop.table(table(bowl_sample_1$color))
    ```

* Is the true value $p$ contained in a CI based on this particular $\hat{p}=0.42$?

* Let's construct a boostrap sample and find out!
]

---

# Constructing a CI based on $\hat{p}$

.pull-left[
```{r}
sample_1_bootstrap <- bowl_sample_1 %>% 
  # given `color` is a factor, tell what is success
  specify(response = color, success = "red") %>% 
  # generate 1000 bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>% 
  # for each, calculate the proportion of successes
  calculate(stat = "prop")

percentile_ci_1 <- sample_1_bootstrap %>% 
  get_confidence_interval(level = 0.95, type = "percentile")
percentile_ci_1
```

* Here to visualize it:
```{r,eval = FALSE}
sample_1_bootstrap %>% 
  visualize(bins = 15) + 
  shade_confidence_interval(endpoints = percentile_ci_1) +
  geom_vline(xintercept = 0.375, linetype = "dashed")
```
]

--

.pull-right[
```{r,fig.height=5,echo = FALSE}
sample_1_bootstrap %>% 
  visualize(bins = 15) + 
  shade_confidence_interval(endpoints = percentile_ci_1) +
  geom_vline(xintercept = 0.375, linetype = "dashed")
```
* Had we taken another sample, we would have obtained a different result. 
* Take 100 samples and see how well we do!
]

---

# How many CIs Contain the True Values?



.left-thin[
1. Take 100 virtual samples from `bowl`.

2. Construct a 95%-CI from each.

3. We plot the true $p = 0.375$.

4. You see that 5 of them miss the true value.

5. Hence, a 95% confidence interval.
]

--

.right-wide[
```{r,echo=FALSE,warning = FALSE,message = FALSE,fig.height=5,fig.width=8}
p_red = 0.375
library(purrr)
library(tidyr)
if(!file.exists("../rds/balls_percentile_cis.rds")){
  set.seed(4)

  # Function to run infer pipeline
  bootstrap_pipeline <- function(sample_data){
    sample_data %>% 
      specify(formula = color ~ NULL, success = "red") %>% 
      generate(reps = 1000, type = "bootstrap") %>% 
      calculate(stat = "prop")
  }
  
  # Compute nested data frame with sampled data, sample proportions, all 
  # bootstrap replicates, and percentile_ci
  balls_percentile_cis <- bowl %>% 
    rep_sample_n(size = 50, reps = 100, replace = FALSE) %>% 
    group_by(replicate) %>% 
    nest() %>% 
    mutate(sample_prop = map_dbl(data, ~mean(.x$color == "red"))) %>%
    # run infer pipeline on each nested tibble to generated bootstrap replicates
    mutate(bootstraps = map(data, bootstrap_pipeline)) %>% 
    group_by(replicate) %>% 
    # Compute 95% percentile CI's for each nested element
    mutate(percentile_ci = map(bootstraps, get_ci, type = "percentile", level = 0.95))
  
  # Save output to rds object
  saveRDS(object = balls_percentile_cis, "../rds/balls_percentile_cis.rds")
} else {
  balls_percentile_cis <- readRDS("../rds/balls_percentile_cis.rds")
}

# Identify if confidence interval captured true p
percentile_cis <- balls_percentile_cis %>% 
  unnest(percentile_ci) %>% 
  mutate(captured = `2.5%` <= p_red & p_red <= `97.5%`)
    
# Plot them!
ggplot(percentile_cis) +
  geom_segment(aes(
    y = replicate, yend = replicate, x = `2.5%`, xend = `97.5%`, 
    alpha = factor(captured, levels = c("TRUE", "FALSE"))
  )) +
  # Removed point estimates since it doesn't necessarily act as center for 
  # percentile-based CI's
  # geom_point(aes(x = sample_prop, y = replicate, color = captured)) +
  labs(x = expression("Proportion of red balls"), y = "Confidence interval number", 
       alpha = "Contains Truth") +
  geom_vline(xintercept = p_red, color = "red") + 
  coord_cartesian(xlim = c(0.1, 0.7)) + 
  theme_light() + 
  theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())
```
]


---

class: title-slide-final, middle

# THANKS

To the amazing [moderndive](https://moderndive.com/) team!

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:florian.oswald@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | florian.oswald@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

